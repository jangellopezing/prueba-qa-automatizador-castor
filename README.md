# Spotify Web - Automated Testing Project

## ğŸ“‹ DescripciÃ³n
Proyecto de automatizaciÃ³n de pruebas funcionales sobre Spotify Web utilizando Java, Serenity BDD, Selenium WebDriver y Cucumber con patrÃ³n Screenplay.

---

## ğŸ”§ Dependencias necesarias

- **JDK 17**
- **Maven 3.9.9+**

**Verificar instalaciÃ³n:**
```bash
java -version
mvn -version
```

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/jangellopezing/prueba-qa-automatizador-castor.git
cd prueba-qa-automatizador-castor
```

### 2. Instalar dependencias
```bash
mvn clean install -DskipTests
```

### 3. Ejecutar las pruebas
```bash
mvn clean verify
```

---

## ğŸ“Š GeneraciÃ³n de reporte

### Generar reportes
Los reportes se generan automÃ¡ticamente despuÃ©s de ejecutar `mvn clean verify`.

### Abrir el reporte
```bash
# El reporte se encuentra en:
target/site/serenity/index.html
```

**Abrir en navegador:**
- Navegar a `target/site/serenity/` y abrir `index.html`

---

## ğŸ§ª Escenarios automatizados

1. **Login con credenciales invÃ¡lidas**
2. **BÃºsqueda de canciones/artistas** ("Coldplay")
3. **NavegaciÃ³n a playlists populares** (Top Listas)

---

## ğŸ“ˆ CÃ³mo interpretar el reporte de resultados

### Vista General del Reporte

El reporte de Serenity se encuentra en `target/site/serenity/index.html` y presenta las siguientes secciones clave:

#### 1. **Feature Coverage (Cobertura de Funcionalidades)**
- **GrÃ¡fico circular** que muestra el porcentaje de escenarios ejecutados
- **67%** indica la cobertura actual de los casos de prueba
- Colores:
    - ğŸŸ¢ Verde: Test Cases que pasaron
    - ğŸ”´ Rojo: Test Cases que fallaron

<img src="src/docs/images/feature_coverage.png" width="300">

#### 2. **Test Outcomes (Resultados de Pruebas)**
Muestra el resumen de ejecuciÃ³n:
- **Passing**: NÃºmero de pruebas exitosas
- **Failed**: NÃºmero de pruebas fallidas

**Ejemplo de lectura:**
- 2 pruebas pasaron exitosamente âœ…
- 1 prueba fallÃ³ âŒ
- 0 pruebas pendientes

<img src="src/docs/images/test_outcomes.png" width="300">

#### 3. **Test Performance (Rendimiento)**
- GrÃ¡fico de barras con el tiempo de ejecuciÃ³n por prueba
- Identifica pruebas lentas que necesitan optimizaciÃ³n
- **DuraciÃ³n Total**: 16s
- **Prueba mÃ¡s rÃ¡pida**: 8s
- **Prueba mÃ¡s lenta**: 16s

<img src="src/docs/images/test_performance.png" width="300">

#### 4. **Key Statistics (EstadÃ­sticas Clave)**
InformaciÃ³n detallada sobre la ejecuciÃ³n:

| MÃ©trica | DescripciÃ³n | Ejemplo |
|---------|-------------|---------|
| **Number of Scenarios** | Total de escenarios ejecutados | 3 |
| **Total Test Cases** | Casos de prueba totales | 3 |
| **Tests Started** | Hora de inicio | Nov 15, 2025 23:54:52 |
| **Tests Finished** | Hora de finalizaciÃ³n | Nov 15, 2025 23:55:09 |
| **Total Duration** | Tiempo total de ejecuciÃ³n | 16s |
| **Average Execution Time** | Tiempo promedio por prueba | 11s |

#### 5. **Automated Tests (Detalle de Pruebas)**
Tabla detallada con cada escenario:

**Columnas:**
- **Feature**: Funcionalidad probada
- **Scenario**: Escenario especÃ­fico
- **Steps**: NÃºmero de pasos ejecutados
- **Started**: Hora de inicio
- **Total Duration**: DuraciÃ³n del test
- **Result**: Estado final (ğŸŸ¢ Success / ğŸ”´ Failed)

<img src="src/docs/images/automated_scenarios.png">

### NavegaciÃ³n en el Reporte

1. **Click en un escenario** â†’ Ver detalles paso a paso con screenshots
2. **Click en "Failed"** â†’ Ver el error especÃ­fico y stack trace
3. **PestaÃ±a "Requirements"** â†’ Ver cobertura por features
4. **PestaÃ±a "Test Results"** â†’ Ver todos los resultados organizados

### AnÃ¡lisis de Resultados

- âœ… **Pruebas exitosas**: El comportamiento esperado funciona correctamente
- âŒ **Pruebas fallidas**: Requieren anÃ¡lisis inmediato
    - Verificar locators
    - Revisar cambios en la UI
    - Validar precondiciones
    - Analizar el stack trace en el reporte

**Ejemplo de anÃ¡lisis de fallo:**
Si "Login con credenciales invÃ¡lidas" falla, revisar si:
1. El mensaje de error estÃ¡ siendo mostrado correctamente
2. Los locators del mensaje de error son correctos
3. El timeout es suficiente para que aparezca el elemento
4. Ver screenshot del momento del fallo en el reporte

---

## ğŸŒ Evidencia del reporte en lÃ­nea (CI/CD)

El proyecto utiliza **GitHub Actions** para la ejecuciÃ³n automÃ¡tica de las pruebas.

ğŸ“Š **Reporte actualizado:** [https://jangellopezing.github.io/prueba-qa-automatizador-castor/](https://jangellopezing.github.io/prueba-qa-automatizador-castor/)

El reporte se genera y se publica automÃ¡ticamente en GitHub Pages despuÃ©s de cada ejecuciÃ³n del pipeline.

---

## ğŸ‘¤ Autor
**Jose Angel Lopez - QA Automation**  
jangel.lopez.ing@gmail.com